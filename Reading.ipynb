{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "flow",
   "display_name": "flow",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\Klemens\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 476
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import pandas as pd\n",
    "import nltk \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"HeadlineData.xlsx\")\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method NDFrame.head of         headline_nr                                           headline label\n0                 0  There Were 2 Mass Shootings In Texas Last Week...     N\n1                 1  Will Smith Joins Diplo And Nicky Jam For The 2...     O\n2                 2    Hugh Grant Marries For The First Time At Age 57     O\n3                 3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...     O\n4                 4  Julianna Margulies Uses Donald Trump Poop Bags...     O\n...             ...                                                ...   ...\n150146       150146                                   Complicated Cuba     o\n150147       150147               Your Pets, Eating Ice Cream (PHOTOS)     p\n150148       150148  First Look' Photos That Will Make You Believe ...     p\n150149       150149      Recommendations for Avoiding Toxic Pet Treats     o\n150150       150150   The Best Fast-Food Burger: A HuffPost Deathmatch     o\n\n[2021 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method NDFrame.head of         headline_nr                                           headline label\n0                 0  there were 2 mass shootings in texas last week...     n\n1                 1  will smith joins diplo and nicky jam for the 2...     o\n2                 2    hugh grant marries for the first time at age 57     o\n3                 3  jim carrey blasts 'castrato' adam schiff and d...     o\n4                 4  julianna margulies uses donald trump poop bags...     o\n...             ...                                                ...   ...\n150146       150146                                   complicated cuba     o\n150147       150147               your pets, eating ice cream (photos)     p\n150148       150148  first look' photos that will make you believe ...     p\n150149       150149      recommendations for avoiding toxic pet treats     o\n150150       150150   the best fast-food burger: a huffpost deathmatch     o\n\n[2021 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "df[\"label\"] = df[\"label\"].str.lower()\n",
    "df[\"headline\"] = df[\"headline\"].str.lower()\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method NDFrame.head of         headline_nr                                           headline label\n0                 0  there were  mass shootings in texas last week ...     n\n1                 1  will smith joins diplo and nicky jam for the  ...     o\n2                 2      hugh grant marries for the first time at age      o\n3                 3  jim carrey blasts castrato adam schiff and dem...     o\n4                 4  julianna margulies uses donald trump poop bags...     o\n...             ...                                                ...   ...\n150146       150146                                   complicated cuba     o\n150147       150147                  your pets eating ice cream photos     p\n150148       150148  first look photos that will make you believe i...     p\n150149       150149      recommendations for avoiding toxic pet treats     o\n150150       150150    the best fast-food burger a huffpost deathmatch     o\n\n[2021 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "df['headline'] = df['headline'].str.replace(',', '')\n",
    "df['headline'] = df['headline'].str.replace(':', '')\n",
    "df['headline'] = df['headline'].str.replace('.', '')\n",
    "df['headline'] = df['headline'].str.replace(';', '')\n",
    "df['headline'] = df['headline'].str.replace(\"'\", '')\n",
    "df['headline'] = df['headline'].str.replace('?', '')\n",
    "df['headline'] = df['headline'].str.replace('!', '')\n",
    "df['headline'] = df['headline'].str.replace('*', '')\n",
    "df['headline'] = df['headline'].str.replace('(', '')\n",
    "df['headline'] = df['headline'].str.replace(')', '')\n",
    "df['headline'] = df['headline'].str.replace('/', '')\n",
    "df['headline'] = df['headline'].str.replace(\"`\", '')\n",
    "df['headline'] = df['headline'].str.replace(\"´\", '')\n",
    "df['headline'] = df['headline'].str.replace(\"‘\", '')\n",
    "df['headline'] = df['headline'].str.replace(\"’\", '')\n",
    "df['headline'] = df['headline'].str.replace('\\d+', '')\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"headline\"] = df[\"headline\"].str.split(pat=' ')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0         [there, were, , mass, shoot, in, texa, last, w...\n1         [will, smith, join, diplo, and, nicki, jam, fo...\n2         [hugh, grant, marri, for, the, first, time, at...\n3         [jim, carrey, blast, castrato, adam, schiff, a...\n4         [julianna, marguli, us, donald, trump, poop, b...\n                                ...                        \n150146                                      [complic, cuba]\n150147                  [your, pet, eat, ice, cream, photo]\n150148    [first, look, photo, that, will, make, you, be...\n150149           [recommend, for, avoid, toxic, pet, treat]\n150150    [the, best, fast-food, burger, a, huffpost, de...\nName: headline, Length: 2021, dtype: object\n"
     ]
    }
   ],
   "source": [
    "lemmatizedwords= []\n",
    "#for ä in df[\"headline\"]:\n",
    " #   for word in ä:\n",
    "  #      word= porter.stem(word)\n",
    "   #     word = lemmatizer.lemmatize(word))\n",
    "df['headline'] = df['headline'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x]) \n",
    "df['headline'] = df['headline'].apply(lambda x: [porter.stem(y) for y in x]) \n",
    "\n",
    "print(df['headline'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTraining=df[:1600]\n",
    "dfTesting=df[1600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method NDFrame.head of         headline_nr                                           headline label\n0                 0  [there, were, , mass, shoot, in, texa, last, w...     n\n1                 1  [will, smith, join, diplo, and, nicki, jam, fo...     o\n2                 2  [hugh, grant, marri, for, the, first, time, at...     o\n3                 3  [jim, carrey, blast, castrato, adam, schiff, a...     o\n4                 4  [julianna, marguli, us, donald, trump, poop, b...     o\n...             ...                                                ...   ...\n108226       108226  [the, seemingly-virtu, often-danger, word, we,...     o\n108227       108227     [game-chang, play, from, week, , in, the, nfl]     o\n108228       108228  [the, art, of, grow, old, how, doe, anyon, mas...     p\n108229       108229  [koko, jone, former, percussionist, for, whitn...     o\n108230       108230            [can, we, make, peopl, want, to, chang]     o\n\n[1600 rows x 3 columns]>\n<bound method NDFrame.head of         headline_nr                                           headline label\n108231       108231  [my, mother, re-form, her, rock, band, --, and...     p\n108232       108232  [whi, the, stori, of, muhammad, ali, rebellion...     p\n108233       108233         [process, rule, are, made, to, be, broken]     p\n108234       108234  [, thing, i, didnt, need, to, rais, a, child, ...     o\n108235       108235  [project, , a, portrait, of, millenni, artist,...     o\n...             ...                                                ...   ...\n150146       150146                                    [complic, cuba]     o\n150147       150147                [your, pet, eat, ice, cream, photo]     p\n150148       150148  [first, look, photo, that, will, make, you, be...     p\n150149       150149         [recommend, for, avoid, toxic, pet, treat]     o\n150150       150150  [the, best, fast-food, burger, a, huffpost, de...     o\n\n[421 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(dfTraining.head)\n",
    "print(dfTesting.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(dfTraining):\n",
    "    for i in dfTraining[\"headline\"]:\n",
    "        for word in i:\n",
    "            if word in Vocabulary:\n",
    "                continue\n",
    "            else:\n",
    "                Vocabulary.append(word)\n",
    "\n",
    "    TrainingDocs = len(dfTraining[\"headline\"])\n",
    "    VSize = len(Vocabulary)\n",
    "    print(\"Training with \",TrainingDocs,\" Headlines and \", VSize,\" unique Words\")\n",
    "\n",
    "    #Zähle ANzahl der Headlines Pro Klasse und speichere sie in eigenem Dataframe\n",
    "    TrainingPosAmount = dfTraining[dfTraining[\"label\"]==\"p\"].count()\n",
    "    TrainingNegAmount = dfTraining[dfTraining[\"label\"]==\"n\"].count()\n",
    "    TrainingNeuAmount = dfTraining[dfTraining[\"label\"]==\"o\"].count()\n",
    "\n",
    "    #Splitte den ursprünglichen Dataframe nach Klasse\n",
    "    TrainingPos = dfTraining[dfTraining[\"label\"]==\"p\"]\n",
    "    TrainingNeg = dfTraining[dfTraining[\"label\"]==\"n\"]\n",
    "    TrainingNeu = dfTraining[dfTraining[\"label\"]==\"o\"]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"Positive: \",TrainingPosAmount[\"label\"])\n",
    "    print(\"Negative: \",TrainingNegAmount[\"label\"])\n",
    "    print(\"Neutral: \",TrainingNeuAmount[\"label\"])\n",
    "\n",
    "    #Berechne Priori Wahrscheinlichkeiten Pro Klasse\n",
    "    PriorPos = TrainingPosAmount[\"label\"]/TrainingDocs\n",
    "    PriorNeg = TrainingNegAmount[\"label\"]/TrainingDocs\n",
    "    PriorNeu = TrainingNeuAmount[\"label\"]/TrainingDocs\n",
    "\n",
    "    PosteriorPos ={}\n",
    "    PosteriorNeg ={}\n",
    "    PosteriorNeu ={}\n",
    "\n",
    "\n",
    "    PositiveWords= []\n",
    "    NegativeWords= []\n",
    "    NeutralWords = []\n",
    "\n",
    "    #Erstelle Listen mit allen Wörtern pro klasse\n",
    "    for x in TrainingPos[\"headline\"]:\n",
    "        for posword in x:\n",
    "            PositiveWords.append(posword)\n",
    "\n",
    "    for h in TrainingNeg[\"headline\"]:\n",
    "        for negword in h:\n",
    "            NegativeWords.append(negword)\n",
    "\n",
    "\n",
    "    for k in TrainingNeu[\"headline\"]:\n",
    "        for neuword in k:\n",
    "            NeutralWords.append(neuword)\n",
    "\n",
    "\n",
    "    #Erstelle Dictionaries mit allen vorkommenden Wörtern und ihrer jeweieiligen Anzahl in jeder der Drei Klassen\n",
    "    for y in Vocabulary:\n",
    "        AmountPos = PositiveWords.count(y)\n",
    "        PosteriorPos[y] = AmountPos\n",
    "\n",
    "\n",
    "    for r in Vocabulary:\n",
    "        AmountNeg = PositiveWords.count(r)\n",
    "        PosteriorNeg[r] = AmountNeg\n",
    "\n",
    "    for q in Vocabulary:\n",
    "        AmountNeu = PositiveWords.count(q)\n",
    "        PosteriorNeu[q] = AmountNeu\n",
    "\n",
    "   #Gib Priori Wahrscheinlichkeiten Pro Klasse aus\n",
    "    print(PriorNeu)\n",
    "    print(PriorNeg)\n",
    "    print(PriorPos)\n",
    "\n",
    "    #Berechne Posterior Wahrscheinlichkeiten pro Wort und Klasse und ersetze die vorher gezählte ANzahl des Vorkommens pro Wort in einer Klasse damit\n",
    "    for g in PosteriorPos:\n",
    "        if PosteriorPos[g] == 0:\n",
    "            PosteriorPos[g] = 0\n",
    "        else:\n",
    "            PosteriorPos[g] = PosteriorPos[g]/len(PositiveWords)\n",
    "\n",
    "    for f in PosteriorNeg:\n",
    "        if PosteriorNeg[f] == 0:\n",
    "            PosteriorNeg[f] = 0\n",
    "        else:\n",
    "            PosteriorNeg[f] = PosteriorNeg[f]/len(NegativeWords)\n",
    "\n",
    "    for ü in PosteriorNeu:\n",
    "        if PosteriorNeu[ü] == 0:\n",
    "            PosteriorNeu[ü] = 0\n",
    "        else:\n",
    "            PosteriorNeu[ü] = PosteriorNeu[ü]/len(NeutralWords)\n",
    "\n",
    "    #print(PosteriorPos)\n",
    "    return Vocabulary, PriorNeg, PriorNeu, PriorPos, PosteriorPos, PosteriorNeg, PosteriorNeu \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0              there\n1               will\n2               hugh\n3                jim\n4           julianna\n             ...    \n108226           the\n108227    game-chang\n108228           the\n108229          koko\n108230           can\nName: headline, Length: 1600, dtype: object\n"
     ]
    }
   ],
   "source": [
    "Vocabulary =[]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training with  1600  Headlines and  4222  unique Words\n",
      "Positive:  331\n",
      "Negative:  595\n",
      "Neutral:  674\n",
      "0.42125\n",
      "0.371875\n",
      "0.206875\n"
     ]
    }
   ],
   "source": [
    "Vocabulary, PriorNeg, PriorNeu, PriorPos, PosteriorPos, PosteriorNeg, PosteriorNeu  = train_naive_bayes(dfTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5333\n"
     ]
    }
   ],
   "source": [
    "print(V_Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(dfTesting, PosteriorPos, PosteriorNeg, PosteriorNeu, PriorNeg, PriorNeu, PriorPos):\n",
    "    AmountGood = 0\n",
    "    AmountBad = 0\n",
    "    AmountNeutral = 0\n",
    "    for g in dfTesting[\"headline\"]:\n",
    "        SumGood = 0\n",
    "        SumBad = 0\n",
    "        SumNeu = 0\n",
    "        for word in g:\n",
    "            if word in Vocabulary:\n",
    "                SumGood += PosteriorPos[word]\n",
    "                SumBad += PosteriorNeg[word]\n",
    "                SumNeu += PosteriorNeu[word]\n",
    "\n",
    "        ProbGood = PriorPos + SumGood\n",
    "        ProbBad = PriorNeg + SumBad\n",
    "        ProbNeu = PriorNeu + SumNeu\n",
    "\n",
    "        if max(ProbGood, ProbBad, ProbNeu) == ProbNeu:\n",
    "           # print(\"I predict neutral for \",g,\" with a Value of \",ProbNeu,\". Probability for Bad is: \",ProbBad,\" and Probablility for good is: \",ProbGood)\n",
    "            AmountNeutral += 1\n",
    "\n",
    "\n",
    "        if max(ProbGood, ProbBad, ProbNeu) == ProbGood:\n",
    "            #print(\"I predict good for \",g,\" with a Value of \",ProbGood,\". Probability for Bad is: \",ProbBad,\" and Probablility for neutral is: \",ProbNeu)\n",
    "            AmountGood += 1\n",
    "\n",
    "        if max(ProbGood, ProbBad, ProbNeu) == ProbBad:\n",
    "            #print(\"I predict bad for \",g,\" with a Value of \",ProbBad,\". Probability for Good is: \",ProbGood,\" and Probablility for neutral is: \",ProbNeu)\n",
    "            AmountBad += 1\n",
    "\n",
    "\n",
    "    print(AmountGood)\n",
    "    print(AmountBad)\n",
    "    print(AmountNeutral)\n",
    "        \n",
    "\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n0\n421\n"
     ]
    }
   ],
   "source": [
    "test_naive_bayes(dfTesting, PosteriorPos, PosteriorNeg, PosteriorNeu, PriorNeg, PriorNeu, PriorPos)"
   ]
  }
 ]
}