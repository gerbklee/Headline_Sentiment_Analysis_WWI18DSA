{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "bcd9ec4b19b962b9852edbc43841838e7a880596d898eccc91c447bc769a64ba"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"HeadlineData.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method NDFrame.head of         headline_nr                                           headline label\n0                 0  There Were 2 Mass Shootings In Texas Last Week...     N\n1                 1  Will Smith Joins Diplo And Nicky Jam For The 2...     O\n2                 2    Hugh Grant Marries For The First Time At Age 57     O\n3                 3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...     O\n4                 4  Julianna Margulies Uses Donald Trump Poop Bags...     O\n...             ...                                                ...   ...\n150146       150146                                   Complicated Cuba     o\n150147       150147               Your Pets, Eating Ice Cream (PHOTOS)     p\n150148       150148  First Look' Photos That Will Make You Believe ...     p\n150149       150149      Recommendations for Avoiding Toxic Pet Treats     o\n150150       150150   The Best Fast-Food Burger: A HuffPost Deathmatch     o\n\n[2021 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method NDFrame.head of         headline_nr                                           headline label\n0                 0  there were 2 mass shootings in texas last week...     n\n1                 1  will smith joins diplo and nicky jam for the 2...     o\n2                 2    hugh grant marries for the first time at age 57     o\n3                 3  jim carrey blasts 'castrato' adam schiff and d...     o\n4                 4  julianna margulies uses donald trump poop bags...     o\n...             ...                                                ...   ...\n150146       150146                                   complicated cuba     o\n150147       150147               your pets, eating ice cream (photos)     p\n150148       150148  first look' photos that will make you believe ...     p\n150149       150149      recommendations for avoiding toxic pet treats     o\n150150       150150   the best fast-food burger: a huffpost deathmatch     o\n\n[2021 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "df[\"label\"] = df[\"label\"].str.lower()\n",
    "df[\"headline\"] = df[\"headline\"].str.lower()\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method NDFrame.head of         headline_nr                                           headline label\n0                 0  there were  mass shootings in texas last week ...     n\n1                 1  will smith joins diplo and nicky jam for the  ...     o\n2                 2      hugh grant marries for the first time at age      o\n3                 3  jim carrey blasts castrato adam schiff and dem...     o\n4                 4  julianna margulies uses donald trump poop bags...     o\n...             ...                                                ...   ...\n150146       150146                                   complicated cuba     o\n150147       150147                  your pets eating ice cream photos     p\n150148       150148  first look photos that will make you believe i...     p\n150149       150149      recommendations for avoiding toxic pet treats     o\n150150       150150    the best fast-food burger a huffpost deathmatch     o\n\n[2021 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "df['headline'] = df['headline'].str.replace(',', '')\n",
    "df['headline'] = df['headline'].str.replace(':', '')\n",
    "df['headline'] = df['headline'].str.replace('.', '')\n",
    "df['headline'] = df['headline'].str.replace(';', '')\n",
    "df['headline'] = df['headline'].str.replace(\"'\", '')\n",
    "df['headline'] = df['headline'].str.replace('?', '')\n",
    "df['headline'] = df['headline'].str.replace('!', '')\n",
    "df['headline'] = df['headline'].str.replace('*', '')\n",
    "df['headline'] = df['headline'].str.replace('(', '')\n",
    "df['headline'] = df['headline'].str.replace(')', '')\n",
    "df['headline'] = df['headline'].str.replace('/', '')\n",
    "df['headline'] = df['headline'].str.replace(\"`\", '')\n",
    "df['headline'] = df['headline'].str.replace(\"´\", '')\n",
    "df['headline'] = df['headline'].str.replace(\"‘\", '')\n",
    "df['headline'] = df['headline'].str.replace(\"’\", '')\n",
    "df['headline'] = df['headline'].str.replace('\\d+', '')\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"headline\"] = df[\"headline\"].str.split(pat=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method NDFrame.head of         headline_nr                                           headline label\n0                 0  [there, were, , mass, shootings, in, texas, la...     n\n1                 1  [will, smith, joins, diplo, and, nicky, jam, f...     o\n2                 2  [hugh, grant, marries, for, the, first, time, ...     o\n3                 3  [jim, carrey, blasts, castrato, adam, schiff, ...     o\n4                 4  [julianna, margulies, uses, donald, trump, poo...     o\n...             ...                                                ...   ...\n150146       150146                                [complicated, cuba]     o\n150147       150147           [your, pets, eating, ice, cream, photos]     p\n150148       150148  [first, look, photos, that, will, make, you, b...     p\n150149       150149  [recommendations, for, avoiding, toxic, pet, t...     o\n150150       150150  [the, best, fast-food, burger, a, huffpost, de...     o\n\n[2021 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTraining=df[:1600]\n",
    "dfTesting=df[1600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method NDFrame.head of         headline_nr                                           headline label\n0                 0  [there, were, , mass, shootings, in, texas, la...     n\n1                 1  [will, smith, joins, diplo, and, nicky, jam, f...     o\n2                 2  [hugh, grant, marries, for, the, first, time, ...     o\n3                 3  [jim, carrey, blasts, castrato, adam, schiff, ...     o\n4                 4  [julianna, margulies, uses, donald, trump, poo...     o\n...             ...                                                ...   ...\n108226       108226  [the, seemingly-virtuous, often-dangerous, wor...     o\n108227       108227  [game-changing, plays, from, week, , in, the, ...     o\n108228       108228  [the, art, of, growing, old, how, does, anyone...     p\n108229       108229  [koko, jones, former, percussionist, for, whit...     o\n108230       108230          [can, we, make, people, want, to, change]     o\n\n[1600 rows x 3 columns]>\n<bound method NDFrame.head of         headline_nr                                           headline label\n108231       108231  [my, mother, re-formed, her, rock, band, --, a...     p\n108232       108232  [why, the, story, of, muhammad, alis, rebellio...     p\n108233       108233        [process, rules, are, made, to, be, broken]     p\n108234       108234  [, things, i, didnt, need, to, raise, a, child...     o\n108235       108235  [project, , a, portrait, of, millennial, artis...     o\n...             ...                                                ...   ...\n150146       150146                                [complicated, cuba]     o\n150147       150147           [your, pets, eating, ice, cream, photos]     p\n150148       150148  [first, look, photos, that, will, make, you, b...     p\n150149       150149  [recommendations, for, avoiding, toxic, pet, t...     o\n150150       150150  [the, best, fast-food, burger, a, huffpost, de...     o\n\n[421 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(dfTraining.head)\n",
    "print(dfTesting.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(dfTraining):\n",
    "    for i in dfTraining[\"headline\"]:\n",
    "        for word in i:\n",
    "            if word in Vocabulary:\n",
    "                continue\n",
    "            else:\n",
    "                Vocabulary.append(word)\n",
    "\n",
    "    TrainingDocs = len(dfTraining[\"headline\"])\n",
    "    VSize = len(Vocabulary)\n",
    "    print(\"Training with \",TrainingDocs,\" Headlines and \", VSize,\" unique Words\")\n",
    "\n",
    "    TrainingPos = dfTraining[dfTraining[\"label\"]==\"p\"].count()\n",
    "    TrainingNeg = dfTraining[dfTraining[\"label\"]==\"n\"].count()\n",
    "    TrainingNeu = dfTraining[dfTraining[\"label\"]==\"o\"].count()\n",
    "    print(\"Positive: \",TrainingPos[\"label\"])\n",
    "    print(\"Negative: \",TrainingNeg[\"label\"])\n",
    "    print(\"Neutral: \",TrainingNeu[\"label\"])\n",
    "\n",
    "    PriorPos = np.log(TrainingPos[\"label\"]/TrainingDocs)\n",
    "    PriorNeg = np.log(TrainingNeg[\"label\"]/TrainingDocs)\n",
    "    PriorNeu = np.log(TrainingNeu[\"label\"]/TrainingDocs)\n",
    "\n",
    "    #for Vword in Vocabulary:\n",
    "        #for line in TrainingPos[\"headline\"]:\n",
    "            #for Element in line:\n",
    "    print(PriorNeu)\n",
    "    print(PriorNeg)\n",
    "    print(PriorPos)\n",
    "\n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0                 there\n1                  will\n2                  hugh\n3                   jim\n4              julianna\n              ...      \n108226              the\n108227    game-changing\n108228              the\n108229             koko\n108230              can\nName: headline, Length: 1600, dtype: object\n"
     ]
    }
   ],
   "source": [
    "Vocabulary =[]\n",
    "print(dfTraining[\"headline\"].str[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training with  1600  Headlines and  5333  unique Words\nPositive:  331\nNegative:  595\nNeutral:  674\n-0.8645287973155655\n-0.9891975026822428\n-1.5756405328508096\n"
     ]
    }
   ],
   "source": [
    "#train_naive_bayes(dfTraining)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataObject:\n",
    "  def __init__(self, data, label):\n",
    "    self.data = data\n",
    "    self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "for index, row in dfTraining.iterrows():\n",
    "    temp = dataObject(row['headline'], row['label'])\n",
    "    training.append(temp)\n",
    "classes = ['p', 'n', 'o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['there', 'were', '', 'mass', 'shootings', 'in', 'texas', 'last', 'week', 'but', 'only', '', 'on', 'tv'] n\n['will', 'smith', 'joins', 'diplo', 'and', 'nicky', 'jam', 'for', 'the', '', 'world', 'cups', 'official', 'song'] o\n['hugh', 'grant', 'marries', 'for', 'the', 'first', 'time', 'at', 'age', ''] o\n['jim', 'carrey', 'blasts', 'castrato', 'adam', 'schiff', 'and', 'democrats', 'in', 'new', 'artwork'] o\n['julianna', 'margulies', 'uses', 'donald', 'trump', 'poop', 'bags', 'to', 'pick', 'up', 'after', 'her', 'dog'] o\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(training[i].data, training[i].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes_sepp(trainingVar, classesVar):\n",
    "    \"\"\"Given a training dataset and the classes that categorize\n",
    "    each observation, return V: a vocabulary of unique words,\n",
    "    logprior: a list of P(c), and loglikelihood: a list of P(fi|c)s\n",
    "    for each word\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    #Initialize D_c[ci]: a list of all documents of class i\n",
    "    #E.g. D_c[1] is a list of [reviews, ratings] of class 1\n",
    "    \n",
    "    #D_c = [[]] * len(classesVar)\n",
    "    D_c = [[] for i in range(len(classesVar))]\n",
    "\n",
    "    #Initialize n_c[ci]: number of documents of class i\n",
    "    n_c = [None] * len(classesVar)\n",
    "\n",
    "    #Initialize logprior[ci]: stores the prior probability for class i\n",
    "    logprior = [None] * len(classesVar)\n",
    "\n",
    "    #Initialize loglikelihood: loglikelihood[ci][wi] stores the likelihood probability for wi given class i\n",
    "    loglikelihood = [None] * len(classesVar)\n",
    "\n",
    "    \n",
    "\n",
    "    #Partition documents into classes. D_c[0]: negative docs, D_c[1]: positive docs\n",
    "    for i in range(len(trainingVar)):    #obs: a [review, rating] pair\n",
    "        #if rating >= 90, classify the review as positive\n",
    "        if trainingVar[i].label == 'p':\n",
    "            temp = [trainingVar[i].data, trainingVar[i].label]\n",
    "            D_c[0].append(temp)    #Can also write as D_c[1] = D_c[1].append(obs)\n",
    "        #else, classify review as negative\n",
    "        elif trainingVar[i].label == 'n':\n",
    "            temp = [trainingVar[i].data, trainingVar[i].label]\n",
    "            D_c[1].append(temp)\n",
    "        elif trainingVar[i].label == 'o':\n",
    "            temp = [trainingVar[i].data, trainingVar[i].label]\n",
    "            D_c[2].append(temp)\n",
    "\n",
    "    #Creates a vocabulary list. For large datasets, this code becomes slow.\n",
    "    #In our post about TF-IDF, we constructed a vocab list that runs much faster.\n",
    "    V = []\n",
    "\n",
    "    for i in range(len(array)):\n",
    "        for aList in array[i]:\n",
    "            for word in aList[0]:\n",
    "                if word in V:\n",
    "                    continue\n",
    "                else:\n",
    "                    V.append(word)\n",
    "    \n",
    "        V_size = len(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "array = train_naive_bayes_sepp(training, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "p\np\np\np\np\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(array[0][i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#Creates a vocabulary list. For large datasets, this code becomes slow.\n",
    "    #In our post about TF-IDF, we constructed a vocab list that runs much faster.\n",
    "V = []\n",
    "\n",
    "for i in range(len(array)):\n",
    "    for aList in array[i]:\n",
    "        for word in aList[0]:\n",
    "            if word in V:\n",
    "                continue\n",
    "            else:\n",
    "                V.append(word)\n",
    "  \n",
    "    V_size = len(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5333"
      ]
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "source": [
    "len(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[331, 595, 674]\n1600\n"
     ]
    }
   ],
   "source": [
    "classesVar = ['p', 'n', 'o']\n",
    "\n",
    "#D_c = [[]] * len(classesVar)\n",
    "D_c = array\n",
    "\n",
    "#Initialize n_c[ci]: number of documents of class i\n",
    "n_c = [None] * len(classesVar)\n",
    "\n",
    "#Initialize logprior[ci]: stores the prior probability for class i\n",
    "logprior = [None] * len(classesVar)\n",
    "\n",
    "#Initialize loglikelihood: loglikelihood[ci][wi] stores the likelihood probability for wi given class i\n",
    "loglikelihood = [None] * len(classesVar)\n",
    "\n",
    "#n_docs: total number of documents in training set\n",
    "n_docs = len(training)\n",
    "\n",
    "n_c[0] = len(D_c[0])\n",
    "n_c[1] = len(D_c[1])\n",
    "n_c[2] = len(D_c[2])\n",
    "\n",
    "print(n_c)\n",
    "print(n_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ci in range(len(classes)):\n",
    "    #Store n_c value for each class\n",
    "    n_c[ci] = len(D_c[ci])\n",
    "\n",
    "    #Compute P(c)\n",
    "    logprior[ci] = np.log((n_c[ci] + 1)/ n_docs) #logprior[0] ist log von [Anzahl Objekte in Klasse 0 (positiv) + 1 / Anzahl alle 1600 Trainingsdaten]\n",
    "\n",
    "\n",
    "    #Counts total number of words in class c\n",
    "    count_w_in_V = 0\n",
    "    for d in D_c[ci]:\n",
    "        count_w_in_V = count_w_in_V + len(d[0])\n",
    "    denom = count_w_in_V + V_size\n",
    "\n",
    "    dic = {}\n",
    "    #Compute P(w|c)\n",
    "    for wi in V:\n",
    "        #Count number of times wi appears in D_c[ci]\n",
    "        count_wi_in_D_c = 0\n",
    "        for d in D_c[ci]:\n",
    "            for word in d[0]:\n",
    "                if word == wi:\n",
    "                    count_wi_in_D_c = count_wi_in_D_c + 1\n",
    "        numer = count_wi_in_D_c + 1\n",
    "        dic[wi] = np.log((numer) / (denom))\n",
    "    loglikelihood[c] = dic\n",
    "\n",
    "return (V, logprior, loglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'A': 1, 'B': 2, 'C': 3}\n"
     ]
    }
   ],
   "source": [
    "dict = {'A':1, 'B':2, 'C':3}\n",
    "print(dict)"
   ]
  }
 ]
}